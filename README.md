For detailed explanation, please refer the medium article: [https://medium.com/@ganeshnasrikrishna/building-a-real-end-to-end-data-pipeline-on-gcp-yes-from-scratch-04b5e78ecbf6](https://medium.com/@ganeshnasrikrishna/building-a-real-end-to-end-data-pipeline-on-gcp-yes-from-scratch-04b5e78ecbf6)

## ğŸŒ End-to-End Streaming Data Pipeline on GCP

This project demonstrates a real-world, production-style streaming data pipeline built from scratch on Google Cloud Platform.

The goal is simple:
ingest real-time air quality data â†’ process it reliably â†’ model it for analytics.

No CSVs. No fake generators. Real APIs. Real problems.

## ğŸ—ï¸ High-Level Architecture

WAQI API -> Cloud Function (Gen2) -> Pub/Sub -> Dataflow (Apache Beam â€“ Streaming, Flex Template) -> BigQuery (Bronze) -> dbt (Silver & Gold models)


Cloud Scheduler is used to poll the API hourly (simulating streaming).

Each layer is decoupled, scalable, and failure-isolated.

## ğŸ”„ Process Overview (Tool-Agnostic)

Ingestion

Fetch air-quality data from a public API

Push raw events into a message queue

Buffering & Decoupling

Pub/Sub absorbs spikes and isolates failures

Producers never wait for consumers

Streaming Processing

Events are parsed, validated, and structured

Late data and retries are handled correctly

Storage

Clean, flat records are stored in BigQuery (Bronze layer)

Transformation

dbt applies business logic:

Deduplication

SCD Type 2 dimensions

Analytics-ready aggregates

## ğŸ“¦ Tech Stack

Cloud Functions (Gen2) â€“ API ingestion

Cloud Scheduler â€“ Hourly triggers

Pub/Sub â€“ Messaging backbone

Apache Beam â€“ Stream processing logic

Cloud Dataflow â€“ Managed Beam execution

BigQuery â€“ Data warehouse

dbt (BigQuery adapter) â€“ Transformations (Bronze â†’ Silver â†’ Gold)

GCS & Artifact Registry â€“ Templates & images

## ğŸ§  Key Design Decisions

Schema-first ingestion
Raw payloads are not dumped blindly.
Fields are explicitly extracted and typed in Beam.

Streaming-first mindset
Event time is preserved. Pipelines resume after failure.

Flex Templates for Dataflow
Pipelines are deployed as reusable artifacts â€” not laptop scripts.

Medallion Architecture

Bronze: structured raw events

Silver: deduplicated, cleaned data

Gold: business-ready metrics

ğŸ—‚ï¸ BigQuery Models
Bronze

One row per event

No deduplication

Minimal logic

Silver

Deduplicated using business keys

Incremental models

Stable hashes

Gold

Daily aggregates

Dimension joins

Analytics-friendly tables

## ğŸ§© dbt Highlights

Incremental models

SCD Type 2 dimension for stations

Data quality tests (not_null, unique)

dbt_utils for surrogate keys

## ğŸš€ Deployment Notes

Dataflow pipelines are deployed using Flex Templates

dbt orchestration is intentionally left open-ended

Cloud Composer + service accounts are the intended next step

(Left as an exerciseâ€¦ and a lesson in IAM patience ğŸ˜…)
